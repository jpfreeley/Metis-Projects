{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is this?\n",
    "\n",
    "This is the same file as '02_generate_dataset.ipynb' except it now distorts images from actual number puzzle examples, rather than artificially made ones. Images are distorted to increase the sample size. The source files are contained in the folder 'base_images_true/'. This file takes those images, and exports them into the folder 'distorted_from_true/' after applying multiple distortions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "from hashlib import sha1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.misc import imsave\n",
    "\n",
    "from skimage.filters import gabor, gaussian\n",
    "from skimage.transform import resize, warp\n",
    "from skimage.util import random_noise\n",
    "from skimage.morphology import dilation, erosion, rectangle, diamond, disk\n",
    "from skimage.transform._geometric import ProjectiveTransform\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random string generator for file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_image(folder, image):\n",
    "    suffix = id_generator()\n",
    "    while suffix in UNIQUE_IDS:\n",
    "        suffix = id_generator()\n",
    "    UNIQUE_IDS.add(suffix)\n",
    "    extension = '.jpg'\n",
    "    filename = \"_\".join([folder, suffix]) + extension\n",
    "    output_file = os.path.join(SAVED_IMAGE_DIR, folder, filename)\n",
    "\n",
    "    imsave(output_file, image)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_IMAGE_DIR = 'base_images_true/'\n",
    "SAVED_IMAGE_DIR = 'distorted_from_true/'\n",
    "DIR_LIST = ['0','1','2','3','4','5','6','7','8','9','X','plus','minus','div']\n",
    "IMAGE_SIZE = 28\n",
    "UNIQUE_IDS = set()\n",
    "UNIQUE_IMAGES = set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for cleaning out the folders where new images will be saved to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_saved_filtered_images():\n",
    "    for folder in DIR_LIST:\n",
    "        for image in os.listdir(SAVED_IMAGE_DIR + folder):\n",
    "            image_file = os.path.join(SAVED_IMAGE_DIR, folder, image)\n",
    "            os.remove(image_file)\n",
    "    \n",
    "    #Reinitialize UNIQUE_IDS and UNIQUE_IMAGES\n",
    "    UNIQUE_IDS.clear()\n",
    "    UNIQUE_IMAGES.clear()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not read: base_images_true/0/.DS_Store : cannot identify image file 'base_images_true/0/.DS_Store' - it's ok, skipping.\n",
      "Could not read: base_images_true/1/.DS_Store : cannot identify image file 'base_images_true/1/.DS_Store' - it's ok, skipping.\n",
      "Could not read: base_images_true/2/.DS_Store : cannot identify image file 'base_images_true/2/.DS_Store' - it's ok, skipping.\n",
      "Could not read: base_images_true/3/.DS_Store : cannot identify image file 'base_images_true/3/.DS_Store' - it's ok, skipping.\n",
      "Could not read: base_images_true/4/.DS_Store : cannot identify image file 'base_images_true/4/.DS_Store' - it's ok, skipping.\n",
      "Could not read: base_images_true/5/.DS_Store : cannot identify image file 'base_images_true/5/.DS_Store' - it's ok, skipping.\n",
      "Could not read: base_images_true/6/.DS_Store : cannot identify image file 'base_images_true/6/.DS_Store' - it's ok, skipping.\n",
      "Could not read: base_images_true/7/.DS_Store : cannot identify image file 'base_images_true/7/.DS_Store' - it's ok, skipping.\n",
      "Could not read: base_images_true/8/.DS_Store : cannot identify image file 'base_images_true/8/.DS_Store' - it's ok, skipping.\n",
      "Could not read: base_images_true/9/.DS_Store : cannot identify image file 'base_images_true/9/.DS_Store' - it's ok, skipping.\n",
      "Could not read: base_images_true/X/.DS_Store : cannot identify image file 'base_images_true/X/.DS_Store' - it's ok, skipping.\n",
      "Could not read: base_images_true/plus/.DS_Store : cannot identify image file 'base_images_true/plus/.DS_Store' - it's ok, skipping.\n",
      "Could not read: base_images_true/minus/.DS_Store : cannot identify image file 'base_images_true/minus/.DS_Store' - it's ok, skipping.\n",
      "Could not read: base_images_true/div/.DS_Store : cannot identify image file 'base_images_true/div/.DS_Store' - it's ok, skipping.\n"
     ]
    }
   ],
   "source": [
    "def apply_filters_and_save():\n",
    "    \n",
    "    for folder in DIR_LIST:\n",
    "        dataset = []\n",
    "\n",
    "        #Read each image from the folder in BASE_IMAGE_DIR\n",
    "        for image in os.listdir(BASE_IMAGE_DIR + folder):\n",
    "            image_file = os.path.join(BASE_IMAGE_DIR, folder, image)\n",
    "            try:\n",
    "                image_data = ndi.imread(image_file, mode = 'L')\n",
    "                image_data = resize(image_data, (IMAGE_SIZE,IMAGE_SIZE))\n",
    "                #print(image_data.shape)\n",
    "\n",
    "                assert image_data.shape == (IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "                dataset.append(image_data)\n",
    "            except IOError as e:\n",
    "                print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "        \n",
    "        dataset = np.array(dataset)\n",
    "        \n",
    "        #Apply filters and save each time\n",
    "        #Sha1 function is used for hashing the numpy arrays and preventing\n",
    "        #Exact duplicates of images\n",
    "        for image in dataset:\n",
    "            #default image\n",
    "            save_image(folder, image)\n",
    "            UNIQUE_IMAGES.add(sha1(image))\n",
    "            \n",
    "            \n",
    "            #random_noise\n",
    "            new_image = random_noise(image, 'gaussian')\n",
    "            if sha1(new_image) not in UNIQUE_IMAGES:\n",
    "                save_image(folder, new_image)\n",
    "                UNIQUE_IMAGES.add(sha1(new_image))\n",
    "            \n",
    "            #translate image slightly. inversion is for preventing 0's from filling border\n",
    "            for i in range(-2,3):\n",
    "                for j in range(-2,3):\n",
    "                    new_image = 1-warp(1-image,ProjectiveTransform(np.array([[1, 0, i],[0,1,j],[0,0,1]])))\n",
    "                    if sha1(new_image) not in UNIQUE_IMAGES:\n",
    "                        save_image(folder, new_image)\n",
    "                        UNIQUE_IMAGES.add(sha1(new_image))\n",
    "            \n",
    "            #erosions and dilations\n",
    "            for i in range(1,2):    \n",
    "                #rectangle/squares\n",
    "                for j in range(1,2):\n",
    "                    new_image = erosion(image, rectangle(i,i))\n",
    "                    if sha1(new_image) not in UNIQUE_IMAGES:\n",
    "                        save_image(folder, new_image)\n",
    "                        UNIQUE_IMAGES.add(sha1(new_image))\n",
    "\n",
    "                #circles\n",
    "                new_image = erosion(image, disk(i))\n",
    "                if sha1(new_image) not in UNIQUE_IMAGES:\n",
    "                    save_image(folder, new_image)\n",
    "                    UNIQUE_IMAGES.add(sha1(new_image))\n",
    "            \n",
    "            #gaussian filter\n",
    "            for sig in range(1,11, 1):   \n",
    "                new_image = gaussian(image, sig*.1)\n",
    "                if sha1(new_image) not in UNIQUE_IMAGES:\n",
    "                    save_image(folder, new_image)\n",
    "                    UNIQUE_IMAGES.add(sha1(new_image))\n",
    "                \n",
    "            #image = gabor(image, 2, 1*math.pi/10)[1]\n",
    "\n",
    "clear_saved_filtered_images()\n",
    "apply_filters_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
